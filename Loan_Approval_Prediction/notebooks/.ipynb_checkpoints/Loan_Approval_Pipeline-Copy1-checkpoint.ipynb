{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738c20f8",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction — ML Pipeline Notebook\n",
    "\n",
    "**Goal:** Build a reproducible ML pipeline (preprocessing → model → tuning → save) for the Loan Approval classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, classification_report)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH =\"../data/train.csv\" \n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0a716",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "# minimal target mapping and dependents fix\n",
    "if 'Loan_Status' not in df.columns:\n",
    "    raise KeyError(\"Loan_Status not found. Make sure your CSV includes it.\")\n",
    "\n",
    "df = df.copy()\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y':1,'N':0})\n",
    "if 'Dependents' in df.columns:\n",
    "    df['Dependents'] = df['Dependents'].replace('3+', 3)\n",
    "    df['Dependents'] = pd.to_numeric(df['Dependents'], errors='coerce')\n",
    "\n",
    "print('Loaded:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb3b25",
   "metadata": {},
   "source": [
    "## X and y and column types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857212dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifier if present\n",
    "if 'Loan_ID' in df.columns:\n",
    "    df = df.drop(columns=['Loan_ID'])\n",
    "\n",
    "y = df['Loan_Status']\n",
    "X = df.drop(columns=['Loan_Status'])\n",
    "\n",
    "# Detect columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print('Numeric cols:', numeric_cols)\n",
    "print('Categorical cols:', cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab07a1",
   "metadata": {},
   "source": [
    "## Build preprocessing pipelines\n",
    "Numeric: median imputation + scaling (useful for linear models).  \n",
    "Categorical: mode imputation + one-hot encoding.  \n",
    "We combine them with `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fefa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "print('Preprocessor ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce090fa2",
   "metadata": {},
   "source": [
    "## Define pipeline with classifier placeholder\n",
    "We create a pipeline containing preprocessing and a classifier. We'll tune RandomForest via GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae2dca",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "Use stratify to preserve class ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654b37d",
   "metadata": {},
   "source": [
    "## Quick cross-validation on training set\n",
    "This gives a baseline for how the pipeline performs before tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('CV scores:', np.round(cv_scores,4))\n",
    "print('CV mean:', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5676f",
   "metadata": {},
   "source": [
    "## GridSearchCV (small grid)\n",
    "We tune a small set of Random Forest hyperparameters to keep compute low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [None, 6, 12],\n",
    "    'clf__min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best CV score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23fe00",
   "metadata": {},
   "source": [
    "## Evaluate best model on test set\n",
    "We print common metrics and show a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc80162",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = grid.best_estimator_\n",
    "\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "y_proba = best_pipe.predict_proba(X_test)[:,1] if hasattr(best_pipe, 'predict_proba') else None\n",
    "\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, zero_division=0))\n",
    "print('Recall:', recall_score(y_test, y_pred, zero_division=0))\n",
    "print('F1:', f1_score(y_test, y_pred, zero_division=0))\n",
    "if y_proba is not None:\n",
    "    print('ROC-AUC:', roc_auc_score(y_test, y_proba))\n",
    "\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724f206",
   "metadata": {},
   "source": [
    "## Cross-validated permutation importance (optional)\n",
    "This gives a model-agnostic idea of which features matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance on test set\n",
    "try:\n",
    "    r = permutation_importance(best_pipe, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    feat_names = np.array(best_pipe.named_steps['preprocessor'].transformers_[0][2].tolist() + list(best_pipe.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)))\n",
    "    imp = pd.Series(r.importances_mean, index=feat_names).sort_values(ascending=False)\n",
    "    print(imp.head(15))\n",
    "except Exception as e:\n",
    "    print('Permutation importance failed:', e)\n",
    "    print('You can compute permutation importance after ensuring transformer names match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289dc80",
   "metadata": {},
   "source": [
    "## Save the best pipeline\n",
    "This saves preprocessing + model together so raw data rows can be predicted later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_pipe, 'loan_pipeline_v1.joblib')\n",
    "print('Pipeline saved to loan_pipeline_v1.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdc5ae",
   "metadata": {},
   "source": [
    "## Sample usage\n",
    "After loading the saved pipeline, pass a new raw-record DataFrame with the same columns (no preprocessing required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load and predict on one row (uncomment and edit a sample row to try)\n",
    "# loaded = joblib.load('loan_pipeline_v1.joblib')\n",
    "# sample = X_test.iloc[[0]]  # replace with a new raw row shaped like X\n",
    "# print('Pred:', loaded.predict(sample), 'Prob:', loaded.predict_proba(sample)[:,1] if hasattr(loaded,'predict_proba') else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaab9c",
   "metadata": {},
   "source": [
    "## Notes / Next steps\n",
    "- Expand GridSearch with randomized search for more hyperparameters.  \n",
    "- Consider using class_weight or resampling if classes are imbalanced.  \n",
    "- For production, add validation on data drift and fairness checks (e.g., performance by gender or area)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
